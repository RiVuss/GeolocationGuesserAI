{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import io\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm import tqdm  # For progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 100 samples.\n",
      "Sub-regions mapped: {'Latin America and the Caribbean': 0, 'Australia and New Zealand': 1, 'Western Asia': 2, 'Sub-Saharan Africa': 3, 'Western Europe': 4, 'Southern Asia': 5, 'South-eastern Asia': 6, 'Eastern Europe': 7, 'Eastern Asia': 8, 'Northern Europe': 9, 'Southern Europe': 10, 'Northern America': 11}\n",
      "Training dataset size: 80\n",
      "Testing dataset size: 20\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:00<00:00,  2.21batch/s, loss=2.32]\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.45batch/s, loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: torch.Size([3, 224, 224]), Label: 0\n",
      "Train Loss: 2.6195, Train Accuracy: 0.0625\n",
      "Test Loss: 2.1523, Test Accuracy: 0.2500\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:00<00:00,  3.03batch/s, loss=2.07]\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  7.94batch/s, loss=1.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: torch.Size([3, 224, 224]), Label: 0\n",
      "Train Loss: 2.1333, Train Accuracy: 0.1375\n",
      "Test Loss: 1.9876, Test Accuracy: 0.4000\n",
      "\n",
      "Training complete! Model saved to 'model_2_18.pth' and metrics to /models/testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, limit=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        if limit:  # Limit the dataset to a small number of observations\n",
    "            self.data = self.data.head(limit)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.subregion_mapping = {subregion: idx for idx, subregion in enumerate(self.data['sub-region'].unique())}\n",
    "        self.data['subregion_label'] = self.data['sub-region'].map(self.subregion_mapping)\n",
    "        self.missing_files = []  # List to log missing files\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.data)} samples.\")\n",
    "        print(f\"Sub-regions mapped: {self.subregion_mapping}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            self.missing_files.append(img_path)\n",
    "            return None  # Skip this sample\n",
    "\n",
    "        image = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data.iloc[idx]['subregion_label']\n",
    "\n",
    "        if idx == 0:  # Show one sample for debugging\n",
    "            print(f\"Sample image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Custom collate function to handle None values\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    predictions = []\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip if batch is empty\n",
    "                continue\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())  # Store predictions\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss, train_accuracy, predictions\n",
    "\n",
    "\n",
    "# Testing function\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    predictions = []\n",
    "\n",
    "    with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip if batch is empty\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():  # Ensure gradients are not computed\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())  # Store predictions\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    return test_loss, test_accuracy, predictions\n",
    "\n",
    "def save_epoch(model_name, model, train_loss, test_loss, train_accuracy, test_accuracy, train_predictions, test_predictions, train_df, test_df, subregion_mapping, save_weights=False):\n",
    "    os.makedirs(f'models/{model_name}', exist_ok=True)\n",
    "\n",
    "    # Corrected metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'train_loss': [train_loss],\n",
    "        'test_loss': [test_loss],\n",
    "        'train_accuracy': [train_accuracy],\n",
    "        'test_accuracy': [test_accuracy]\n",
    "    })\n",
    "\n",
    "    # Reverse mapping for human-readable sub-region names\n",
    "    reverse_mapping = {v: k for k, v in subregion_mapping.items()}\n",
    "\n",
    "    # Map numeric predictions to sub-region names\n",
    "    train_df['model_prediction'] = [reverse_mapping[pred] for pred in train_predictions]\n",
    "    test_df['model_prediction'] = [reverse_mapping[pred] for pred in test_predictions]\n",
    "\n",
    "    # Save model weights if required\n",
    "    if save_weights:\n",
    "        torch.save(model.state_dict(), f'models/{model_name}/model.pth')\n",
    "\n",
    "    # Save metrics and predictions to CSV\n",
    "    metrics_df.to_csv(f\"models/{model_name}/metrics.csv\", index=False)\n",
    "    train_df.to_csv(f\"models/{model_name}/train_predictions.csv\", index=False)\n",
    "    test_df.to_csv(f\"models/{model_name}/test_predictions.csv\", index=False)\n",
    "\n",
    "\n",
    "# Main training loop with adjusted DataFrame handling\n",
    "def train_loop(csv_path, root_dir, num_epochs=2, batch_size=2, learning_rate=0.001, weight_decay=0.0001, limit=None):\n",
    "    model_name = input(\"Give model name: \")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data transformations\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "\n",
    "    # Dataset and DataLoaders\n",
    "    dataset = CustomDataset(csv_file=csv_path, root_dir=root_dir, transform=transform, limit=limit)\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Separate train and test DataFrames\n",
    "    train_df = dataset.data.iloc[train_idx].reset_index(drop=True)\n",
    "    test_df = dataset.data.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_set)}\")\n",
    "    print(f\"Testing dataset size: {len(test_set)}\")\n",
    "\n",
    "    # Log missing files\n",
    "    if dataset.missing_files:\n",
    "        print(f\"Missing files: {len(dataset.missing_files)}\")\n",
    "        with open('missing_files.log', 'w') as f:\n",
    "            for file in dataset.missing_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "\n",
    "    # Load ResNet18 and modify the final layer\n",
    "    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, len(dataset.subregion_mapping))  # Number of sub-regions\n",
    "    resnet.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss, train_accuracy, train_predictions = train(resnet, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_accuracy, test_predictions = test(resnet, test_loader, criterion, device)\n",
    "        save_weights = epoch == num_epochs - 1\n",
    "\n",
    "        # Save epoch results\n",
    "        save_epoch(model_name, resnet, train_loss, test_loss, train_accuracy, test_accuracy,\n",
    "                   train_predictions, test_predictions, train_df, test_df, dataset.subregion_mapping, save_weights)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining complete! Model saved to 'model_2_18.pth' and metrics to /models/{model_name}.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Ensure 'coords_processed.csv' and 'Streetview_Image_Dataset/' are properly set up\n",
    "train_loop(csv_path='coords_processed_large_dataset.csv', root_dir='Streetview_Image_Dataset/', num_epochs=2, batch_size=64, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 25229 samples.\n",
      "Sub-regions mapped: {'Latin America and the Caribbean': 0, 'Australia and New Zealand': 1, 'Western Asia': 2, 'Sub-Saharan Africa': 3, 'Western Europe': 4, 'Southern Asia': 5, 'South-eastern Asia': 6, 'Eastern Europe': 7, 'Eastern Asia': 8, 'Northern Europe': 9, 'Southern Europe': 10, 'Northern America': 11, 'Central Asia': 12, nan: 13, 'Northern Africa': 14, 'Melanesia': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_4032\\1140909424.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 25229 samples.\n",
      "Sub-regions mapped: {'Latin America and the Caribbean': 0, 'Australia and New Zealand': 1, 'Western Asia': 2, 'Sub-Saharan Africa': 3, 'Western Europe': 4, 'Southern Asia': 5, 'South-eastern Asia': 6, 'Eastern Europe': 7, 'Eastern Asia': 8, 'Northern Europe': 9, 'Southern Europe': 10, 'Northern America': 11, 'Central Asia': 12, nan: 13, 'Northern Africa': 14, 'Melanesia': 15}\n",
      "Generating predictions...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (5045) does not match length of index (5046)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m save_predictions(\n\u001b[0;32m     71\u001b[0m     csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords_processed_large_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;66;03m# Input CSV with all data\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreetview_Image_Dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m,               \u001b[38;5;66;03m# Path to the dataset folder\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_2_18.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Path to the saved trained model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_predictions_model_2_18.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Output CSV with predictions\u001b[39;00m\n\u001b[0;32m     75\u001b[0m )\n",
      "Cell \u001b[1;32mIn[12], line 57\u001b[0m, in \u001b[0;36msave_predictions\u001b[1;34m(csv_path, root_dir, model_path, output_path)\u001b[0m\n\u001b[0;32m     54\u001b[0m inverse_mapping \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m subregion_mapping\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     56\u001b[0m test_data \u001b[38;5;241m=\u001b[39m full_dataset\u001b[38;5;241m.\u001b[39miloc[test_idx]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 57\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [inverse_mapping[pred] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[0;32m     58\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [inverse_mapping[true_label] \u001b[38;5;28;01mfor\u001b[39;00m true_label \u001b[38;5;129;01min\u001b[39;00m true_labels]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Add probabilities for each category as separate columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4180\u001b[0m     ):\n\u001b[0;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4915\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (5045) does not match length of index (5046)"
     ]
    }
   ],
   "source": [
    "# Save predictions with probabilities for each category\n",
    "def save_predictions(csv_path, root_dir, model_path, output_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, len(CustomDataset(csv_path, root_dir).subregion_mapping))  # Number of sub-regions\n",
    "    resnet.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    resnet.to(device)\n",
    "    resnet.eval()\n",
    "\n",
    "    # Load the test dataset\n",
    "    dataset = CustomDataset(csv_file=csv_path, root_dir=root_dir, transform=transform)\n",
    "    test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)[1]\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    # Custom collate function to handle None values\n",
    "    def collate_fn(batch):\n",
    "        batch = [item for item in batch if item is not None]\n",
    "        if len(batch) == 0:\n",
    "            return None\n",
    "        return torch.utils.data.default_collate(batch)\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    # Create a DataFrame to store predictions and probabilities\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            if batch is None:  # Skip empty batches\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = resnet(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()  # Compute probabilities\n",
    "            predicted_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "            predictions.append(predicted_class)\n",
    "            true_labels.append(labels.item())\n",
    "            probabilities.append(probs.flatten())  # Store all probabilities\n",
    "\n",
    "    # Map numeric labels back to sub-regions\n",
    "    full_dataset = pd.read_csv(csv_path)\n",
    "    subregion_mapping = {subregion: idx for idx, subregion in enumerate(full_dataset['sub-region'].unique())}\n",
    "    inverse_mapping = {v: k for k, v in subregion_mapping.items()}\n",
    "\n",
    "    test_data = full_dataset.iloc[test_idx].copy()\n",
    "    test_data['model_prediction'] = [inverse_mapping[pred] for pred in predictions]\n",
    "    test_data['true_label'] = [inverse_mapping[true_label] for true_label in true_labels]\n",
    "\n",
    "    # Add probabilities for each category as separate columns\n",
    "    for idx, subregion in inverse_mapping.items():\n",
    "        test_data[f'prob_{subregion}'] = [prob[idx] for prob in probabilities]\n",
    "\n",
    "    # Save the test dataset with predictions and probabilities\n",
    "    test_data.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "save_predictions(\n",
    "    csv_path='coords_processed_large_dataset.csv',    # Input CSV with all data\n",
    "    root_dir='Streetview_Image_Dataset/',               # Path to the dataset folder\n",
    "    model_path='model_2_18.pth',            # Path to the saved trained model\n",
    "    output_path='test_predictions_model_2_18.csv' # Output CSV with predictions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
