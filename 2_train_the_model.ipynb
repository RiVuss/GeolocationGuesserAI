{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import io\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm import tqdm  # For progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 9999 samples.\n",
      "Continents mapped: {'North America': 0, 'South America': 1, 'Oceania': 2, 'Asia': 3, 'Africa': 4, 'Europe': 5}\n",
      "Training dataset size: 7999\n",
      "Testing dataset size: 2000\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/125 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "# Custom dataset class with debug prints\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, limit=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        if limit:  # Limit the dataset to a small number of observations\n",
    "            self.data = self.data.head(limit)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.continent_mapping = {continent: idx for idx, continent in enumerate(self.data['continent'].unique())}\n",
    "        self.data['continent_label'] = self.data['continent'].map(self.continent_mapping)\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.data)} samples.\")\n",
    "        print(f\"Continents mapped: {self.continent_mapping}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_name'])\n",
    "        #print(img_path)\n",
    "        image = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data.iloc[idx]['continent_label']\n",
    "\n",
    "        if idx == 0:  # Show one sample for debugging\n",
    "            print(f\"Sample image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Training function with progress display\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "            # Update progress bar with loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "# Testing function with progress display\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "            # Update progress bar with loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Main training loop\n",
    "def train_loop(csv_path, root_dir, num_epochs=2, batch_size=2, learning_rate=0.001, weight_decay=0.0001, limit=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data transformations\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "\n",
    "    # Dataset and DataLoaders (limited to `limit` samples for testing)\n",
    "    dataset = CustomDataset(csv_file=csv_path, root_dir=root_dir, transform=transform, limit=limit)\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_set)}\")\n",
    "    print(f\"Testing dataset size: {len(test_set)}\")\n",
    "\n",
    "    # Load ResNet18 and modify the final layer\n",
    "    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    # for param in resnet.parameters():\n",
    "    #     param.requires_grad = False  # Freeze all layers except the last\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, 6)  # 6 continents\n",
    "    resnet.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training and evaluation\n",
    "    metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'train_accuracy', 'test_loss', 'test_accuracy'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss, train_accuracy = train(resnet, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_accuracy = test(resnet, test_loader, criterion, device)\n",
    "\n",
    "        # Save metrics and model\n",
    "        metrics = metrics.append({'epoch': epoch, 'train_loss': train_loss, 'train_accuracy': train_accuracy,\n",
    "                                  'test_loss': test_loss, 'test_accuracy': test_accuracy}, ignore_index=True)\n",
    "        metrics.to_csv('metrics.csv', index=False)\n",
    "        torch.save(resnet.state_dict(), 'model.pth')\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete! Model saved to 'model.pth' and metrics to 'metrics.csv'.\")\n",
    "\n",
    "# Example usage\n",
    "# Ensure 'coords_processed.csv' and 'dataset/' are properly set up\n",
    "train_loop(csv_path='coords_processed.csv', root_dir='dataset/', num_epochs=15, batch_size=64, limit=9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_11316\\2691431291.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 10000 samples.\n",
      "Continents mapped: {'North America': 0, 'South America': 1, 'Oceania': 2, 'Asia': 3, 'Africa': 4, 'Europe': 5}\n",
      "Generating predictions...\n",
      "Sample image shape: torch.Size([3, 224, 224]), Label: 0\n",
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "def save_predictions(csv_path, root_dir, model_path, output_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, 6)  # 6 continents\n",
    "    resnet.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    resnet.to(device)\n",
    "    resnet.eval()\n",
    "\n",
    "    # Load the test dataset\n",
    "    dataset = CustomDataset(csv_file=csv_path, root_dir=root_dir, transform=transform)\n",
    "    test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)[1]\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Create a DataFrame to store predictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = resnet(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "\n",
    "            predictions.append(preds.item())\n",
    "            true_labels.append(labels.item())\n",
    "\n",
    "    # Map numeric labels back to continents\n",
    "    full_dataset = pd.read_csv(csv_path)\n",
    "    continent_mapping = {continent: idx for idx, continent in enumerate(full_dataset['continent'].unique())}\n",
    "    inverse_mapping = {v: k for k, v in continent_mapping.items()}\n",
    "\n",
    "    test_data = full_dataset.iloc[test_idx].copy()\n",
    "    test_data['model_prediction'] = [inverse_mapping[pred] for pred in predictions]\n",
    "\n",
    "    # Save the test dataset with predictions\n",
    "    test_data.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "save_predictions(\n",
    "    csv_path='coords_processed.csv',    # Input CSV with all data\n",
    "    root_dir='dataset/',               # Path to the dataset folder\n",
    "    model_path='model.pth',            # Path to the saved trained model\n",
    "    output_path='test_predictions.csv' # Output CSV with predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
