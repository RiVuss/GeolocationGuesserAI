{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.io as io\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, resnet101\n",
    "from torchvision.models import ResNet18_Weights, ResNet101_Weights\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 25229 samples.\n",
      "Sub-regions mapped: {'Latin America and the Caribbean': 0, 'Australia and New Zealand': 1, 'Western Asia': 2, 'Sub-Saharan Africa': 3, 'Western Europe': 4, 'Southern Asia': 5, 'South-eastern Asia': 6, 'Eastern Europe': 7, 'Eastern Asia': 8, 'Northern Europe': 9, 'Southern Europe': 10, 'Northern America': 11, 'Central Asia': 12, 'Eastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\nEastern Asia\\r\\n': 13, 'Northern Africa': 14, 'Eastern Asia\\r\\n': 15, 'Melanesia': 16}\n",
      "Training dataset size: 20183\n",
      "Testing dataset size: 5046\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2523/2523 [03:28<00:00, 12.09batch/s, loss=1.73] \n",
      "Testing: 100%|██████████| 631/631 [00:51<00:00, 12.14batch/s, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0436, Train Accuracy: 0.3168\n",
      "Test Loss: 1.8635, Test Accuracy: 0.3855\n",
      "Epoch 1 training time: 260.66 seconds\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2523/2523 [03:21<00:00, 12.51batch/s, loss=1.85] \n",
      "Testing: 100%|██████████| 631/631 [00:46<00:00, 13.71batch/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8986, Train Accuracy: 0.3667\n",
      "Test Loss: 1.9085, Test Accuracy: 0.3744\n",
      "Epoch 2 training time: 247.70 seconds\n",
      "\n",
      "Training complete! Model and metrics saved to /models/test.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 1) Class for using a dataset, including limiting the number of images used\n",
    "####################################\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, limit=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        if limit:  # Limit the dataset to a small number of observations\n",
    "            self.data = self.data.head(limit)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mapping sub-region strings to integer labels\n",
    "        self.subregion_mapping = {\n",
    "            subregion: idx\n",
    "            for idx, subregion in enumerate(self.data['sub-region'].unique())\n",
    "        }\n",
    "        self.data['subregion_label'] = self.data['sub-region'].map(self.subregion_mapping)\n",
    "\n",
    "        self.missing_files = []  # List to log missing files\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.data)} samples.\")\n",
    "        print(f\"Sub-regions mapped: {self.subregion_mapping}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            self.missing_files.append(img_path)\n",
    "            return None  # Skip this sample\n",
    "\n",
    "        image = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data.iloc[idx]['subregion_label']\n",
    "\n",
    "        # Debug: print shape for the first item\n",
    "        # if idx == 0:\n",
    "        #     print(f\"Sample image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "\n",
    "####################################\n",
    "# 2) TRAIN FUNCTION (with softmax)\n",
    "####################################\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # We'll store raw predictions for final column, plus softmax probabilities\n",
    "    predictions = []\n",
    "    probabilities_list = []\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip if batch is empty\n",
    "                continue\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Predictions\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Softmax probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probabilities_list.append(probs.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Combine probabilities from all batches into one array\n",
    "    probabilities_array = np.concatenate(probabilities_list, axis=0)\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return avg_loss, train_accuracy, predictions, probabilities_array\n",
    "\n",
    "####################################\n",
    "# 3) TEST FUNCTION \n",
    "####################################\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    predictions = []\n",
    "    probabilities_list = []\n",
    "\n",
    "    with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Predictions\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Softmax probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probabilities_list.append(probs.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Combine probabilities\n",
    "    probabilities_array = np.concatenate(probabilities_list, axis=0)\n",
    "\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return avg_loss, test_accuracy, predictions, probabilities_array\n",
    "\n",
    "####################################\n",
    "# 4) SAVE EPOCH FUNCTION\n",
    "####################################\n",
    "def save_epoch(\n",
    "    model_name,\n",
    "    model,\n",
    "    train_loss,\n",
    "    test_loss,\n",
    "    train_accuracy,\n",
    "    test_accuracy,\n",
    "    train_predictions,\n",
    "    test_predictions,\n",
    "    train_probabilities,     \n",
    "    test_probabilities,      \n",
    "    train_df,\n",
    "    test_df,\n",
    "    subregion_mapping,\n",
    "    save_weights=False\n",
    "):\n",
    "    os.makedirs(f'models/{model_name}', exist_ok=True)\n",
    "\n",
    "    # Metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'train_loss': [train_loss],\n",
    "        'test_loss': [test_loss],\n",
    "        'train_accuracy': [train_accuracy],\n",
    "        'test_accuracy': [test_accuracy]\n",
    "    })\n",
    "\n",
    "    # Reverse mapping for human-readable sub-region names\n",
    "    reverse_mapping = {v: k for k, v in subregion_mapping.items()}\n",
    "\n",
    "    # Single best-label predictions\n",
    "    train_df['model_prediction'] = [reverse_mapping[pred] for pred in train_predictions]\n",
    "    test_df['model_prediction'] = [reverse_mapping[pred] for pred in test_predictions]\n",
    "\n",
    "    # Add a probability column for each sub-region (e.g., \"prob_<sub-region>\")\n",
    "    for class_idx, class_name in reverse_mapping.items():\n",
    "        col_name_train = f'prob_{class_name}'\n",
    "        col_name_test = f'prob_{class_name}'\n",
    "\n",
    "        # For each row in train_probabilities, store the probability for class_idx\n",
    "        train_df[col_name_train] = [row[class_idx] for row in train_probabilities]\n",
    "        # For test\n",
    "        test_df[col_name_test] = [row[class_idx] for row in test_probabilities]\n",
    "\n",
    "    # Save model weights if required\n",
    "    if save_weights:\n",
    "        torch.save(model.state_dict(), f'models/{model_name}/model.pth')\n",
    "\n",
    "    # Save metrics and predictions to CSV\n",
    "    metrics_df.to_csv(f\"models/{model_name}/metrics.csv\", index=False)\n",
    "    train_df.to_csv(f\"models/{model_name}/train_predictions.csv\", index=False)\n",
    "    test_df.to_csv(f\"models/{model_name}/test_predictions.csv\", index=False)\n",
    "\n",
    "####################################\n",
    "# 6) TRAIN LOOP \n",
    "####################################\n",
    "def train_loop(\n",
    "    csv_path,\n",
    "    root_dir,\n",
    "    num_epochs=2,\n",
    "    batch_size=2,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    limit=None,\n",
    "    resnet_=resnet18,\n",
    "    weights_=ResNet18_Weights,\n",
    "    freeze = False\n",
    "):\n",
    "    # Prompt user for model name\n",
    "    model_name = input(\"Give model name: \")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data transformations\n",
    "    weights = weights_.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "\n",
    "    # Dataset\n",
    "    dataset = CustomDataset(\n",
    "        csv_file=csv_path,\n",
    "        root_dir=root_dir,\n",
    "        transform=transform,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    # Split indices for train/test\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(dataset)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # train/test DataFrames\n",
    "    train_df = dataset.data.iloc[train_idx].reset_index(drop=True)\n",
    "    test_df = dataset.data.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_set)}\")\n",
    "    print(f\"Testing dataset size: {len(test_set)}\")\n",
    "\n",
    "    # Log missing files if any - this may be no longer necessary now that the file is fixed. \n",
    "    if dataset.missing_files:\n",
    "        print(f\"Missing files: {len(dataset.missing_files)}\")\n",
    "        with open('missing_files.log', 'w') as f:\n",
    "            for file in dataset.missing_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "\n",
    "    # Load chosen ResNet\n",
    "    resnet = resnet_(weights=weights_.DEFAULT)\n",
    "\n",
    "    # Freeze all layers except the last\n",
    "    if(freeze == True):\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    \n",
    "    # Modify the final FC layer\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, len(dataset.subregion_mapping))\n",
    "    resnet.to(device)\n",
    "\n",
    "    # Loss function & Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    training_times = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train step (returns probabilities)\n",
    "        train_loss, train_accuracy, train_predictions, train_probabilities = train(\n",
    "            resnet, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        # Test step (returns probabilities)\n",
    "        test_loss, test_accuracy, test_predictions, test_probabilities = test(\n",
    "            resnet, test_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        training_times.append({'epoch': epoch + 1, 'training_time': epoch_time})\n",
    "\n",
    "        # Save everything for this epoch\n",
    "        save_epoch(\n",
    "            model_name, \n",
    "            resnet,\n",
    "            train_loss, \n",
    "            test_loss,\n",
    "            train_accuracy, \n",
    "            test_accuracy,\n",
    "            train_predictions,\n",
    "            test_predictions,\n",
    "            train_probabilities,   \n",
    "            test_probabilities,    \n",
    "            train_df, \n",
    "            test_df, \n",
    "            dataset.subregion_mapping,\n",
    "            save_weights=(epoch == num_epochs - 1)  # only save on last epoch\n",
    "        )\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} training time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # Save training times\n",
    "    training_times_df = pd.DataFrame(training_times)\n",
    "    training_times_df.to_csv(f\"models/{model_name}/training_times.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nTraining complete! Model and metrics saved to /models/{model_name}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loop(\n",
    "    csv_path='coords_processed.csv',\n",
    "    root_dir='Streetview_Image_Dataset/',\n",
    "    num_epochs=2,\n",
    "    batch_size=8,  # for example\n",
    "    resnet_=resnet18, \n",
    "    weights_=ResNet18_Weights,\n",
    "    limit=None,\n",
    "    freeze=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
