{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.io as io\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, resnet101\n",
    "from torchvision.models import ResNet18_Weights, ResNet101_Weights\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>region</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>sub-region</th>\n",
       "      <th>is_augmented</th>\n",
       "      <th>aumentation_source_image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.603220</td>\n",
       "      <td>15.538784</td>\n",
       "      <td>3388.png</td>\n",
       "      <td>HR</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>HR</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>False</td>\n",
       "      <td>3388.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.382881</td>\n",
       "      <td>3.673380</td>\n",
       "      <td>2963.png</td>\n",
       "      <td>NG</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>NG</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>False</td>\n",
       "      <td>2963.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.400360</td>\n",
       "      <td>103.894100</td>\n",
       "      <td>14138.png</td>\n",
       "      <td>MY</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>MY</td>\n",
       "      <td>South-eastern Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>14138.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.949390</td>\n",
       "      <td>-7.846810</td>\n",
       "      <td>23442.png</td>\n",
       "      <td>IE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>IE</td>\n",
       "      <td>Northern Europe</td>\n",
       "      <td>False</td>\n",
       "      <td>23442.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.186400</td>\n",
       "      <td>34.864500</td>\n",
       "      <td>20450.png</td>\n",
       "      <td>IL</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Israel</td>\n",
       "      <td>IL</td>\n",
       "      <td>Western Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>20450.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude image_name country_code   country continent  \\\n",
       "0  45.603220   15.538784   3388.png           HR   Croatia    Europe   \n",
       "1   7.382881    3.673380   2963.png           NG   Nigeria    Africa   \n",
       "2   1.400360  103.894100  14138.png           MY  Malaysia      Asia   \n",
       "3  53.949390   -7.846810  23442.png           IE   Ireland    Europe   \n",
       "4  32.186400   34.864500  20450.png           IL    Israel      Asia   \n",
       "\n",
       "     region alpha-2          sub-region  is_augmented  \\\n",
       "0   Croatia      HR     Southern Europe         False   \n",
       "1   Nigeria      NG  Sub-Saharan Africa         False   \n",
       "2  Malaysia      MY  South-eastern Asia         False   \n",
       "3   Ireland      IE     Northern Europe         False   \n",
       "4    Israel      IL        Western Asia         False   \n",
       "\n",
       "  aumentation_source_image_name  \n",
       "0                      3388.png  \n",
       "1                      2963.png  \n",
       "2                     14138.png  \n",
       "3                     23442.png  \n",
       "4                     20450.png  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset initialized with 120 samples.\n",
      "Sub-regions mapped: {'Southern Europe': 0, 'Sub-Saharan Africa': 1, 'South-eastern Asia': 2, 'Northern Europe': 3, 'Western Asia': 4, 'Latin America and the Caribbean': 5, 'Australia and New Zealand': 6, 'Northern America': 7, 'Eastern Europe': 8, 'Western Europe': 9, 'Southern Asia': 10, 'Eastern Asia': 11}\n",
      "Dataset initialized with 120 samples.\n",
      "Sub-regions mapped: {'Eastern Europe': 0, 'Latin America and the Caribbean': 1, 'Northern America': 2, 'Sub-Saharan Africa': 3, 'Southern Europe': 4, 'South-eastern Asia': 5, 'Eastern Asia': 6, 'Western Europe': 7, 'Western Asia': 8, 'Southern Asia': 9, 'Australia and New Zealand': 10, 'Northern Europe': 11}\n",
      "Training dataset size: 120\n",
      "Testing dataset size: 120\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [00:01<00:00, 10.21batch/s, loss=2.66]\n",
      "Testing: 100%|██████████| 15/15 [00:01<00:00, 12.71batch/s, loss=3.02]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (114) does not match length of index (120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 352\u001b[0m\n\u001b[0;32m    345\u001b[0m     training_times_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/training_times.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete! Model and metrics saved to /models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m train_loop(\n\u001b[0;32m    353\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreetview_Image_Dataset/processed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    354\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    355\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \u001b[38;5;66;03m# for example\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     resnet_\u001b[38;5;241m=\u001b[39mresnet18, \n\u001b[0;32m    357\u001b[0m     weights_\u001b[38;5;241m=\u001b[39mResNet18_Weights,\n\u001b[0;32m    358\u001b[0m     limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[0;32m    359\u001b[0m     freeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     augmented_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "Cell \u001b[1;32mIn[17], line 322\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(root_dir, num_epochs, batch_size, learning_rate, weight_decay, limit, resnet_, weights_, freeze, augmented_data)\u001b[0m\n\u001b[0;32m    319\u001b[0m training_times\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch_time})\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Save everything for this epoch\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m save_epoch(\n\u001b[0;32m    323\u001b[0m     model_name, \n\u001b[0;32m    324\u001b[0m     resnet,\n\u001b[0;32m    325\u001b[0m     train_loss, \n\u001b[0;32m    326\u001b[0m     test_loss,\n\u001b[0;32m    327\u001b[0m     train_accuracy, \n\u001b[0;32m    328\u001b[0m     test_accuracy,\n\u001b[0;32m    329\u001b[0m     train_predictions,\n\u001b[0;32m    330\u001b[0m     test_predictions,\n\u001b[0;32m    331\u001b[0m     train_probabilities,   \n\u001b[0;32m    332\u001b[0m     test_probabilities,    \n\u001b[0;32m    333\u001b[0m     train_df, \n\u001b[0;32m    334\u001b[0m     test_df, \n\u001b[0;32m    335\u001b[0m     train_set\u001b[38;5;241m.\u001b[39msubregion_mapping,\n\u001b[0;32m    336\u001b[0m     save_weights\u001b[38;5;241m=\u001b[39m(epoch \u001b[38;5;241m==\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# only save on last epoch\u001b[39;00m\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 178\u001b[0m, in \u001b[0;36msave_epoch\u001b[1;34m(model_name, model, train_loss, test_loss, train_accuracy, test_accuracy, train_predictions, test_predictions, train_probabilities, test_probabilities, train_df, test_df, subregion_mapping, save_weights)\u001b[0m\n\u001b[0;32m    175\u001b[0m reverse_mapping \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m subregion_mapping\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Single best-label predictions\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [reverse_mapping[pred] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m train_predictions]\n\u001b[0;32m    179\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [reverse_mapping[pred] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m test_predictions]\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# Add a probability column for each sub-region (e.g., \"prob_<sub-region>\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4180\u001b[0m     ):\n\u001b[0;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4915\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user1\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (114) does not match length of index (120)"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 1) Class for using a dataset, including limiting the number of images used\n",
    "####################################\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, limit=None, augmented_data=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        if not augmented_data and \"is_augmented\" in self.data.columns:\n",
    "            self.data = self.data[self.data['is_augmented']==False]\n",
    "\n",
    "        if limit:  # Limit the dataset to a small number of observations\n",
    "            self.data = self.data.head(limit)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mapping sub-region strings to integer labels\n",
    "        self.subregion_mapping = {\n",
    "            subregion: idx\n",
    "            for idx, subregion in enumerate(self.data['sub-region'].unique())\n",
    "        }\n",
    "        self.data['subregion_label'] = self.data['sub-region'].map(self.subregion_mapping)\n",
    "\n",
    "        self.missing_files = []  # List to log missing files\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.data)} samples.\")\n",
    "        print(f\"Sub-regions mapped: {self.subregion_mapping}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            self.missing_files.append(img_path)\n",
    "            return None  # Skip this sample\n",
    "\n",
    "        image = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data.iloc[idx]['subregion_label']\n",
    "\n",
    "        # Debug: print shape for the first item\n",
    "        # if idx == 0:\n",
    "        #     print(f\"Sample image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "\n",
    "####################################\n",
    "# 2) TRAIN FUNCTION (with softmax)\n",
    "####################################\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # We'll store raw predictions for final column, plus softmax probabilities\n",
    "    predictions = []\n",
    "    probabilities_list = []\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:  # Skip if batch is empty\n",
    "                continue\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Predictions\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Softmax probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probabilities_list.append(probs.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Combine probabilities from all batches into one array\n",
    "    probabilities_array = np.concatenate(probabilities_list, axis=0)\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return avg_loss, train_accuracy, predictions, probabilities_array\n",
    "\n",
    "####################################\n",
    "# 3) TEST FUNCTION \n",
    "####################################\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    predictions = []\n",
    "    probabilities_list = []\n",
    "\n",
    "    with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "        for batch in pbar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Predictions\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Softmax probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probabilities_list.append(probs.detach().cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Combine probabilities\n",
    "    probabilities_array = np.concatenate(probabilities_list, axis=0)\n",
    "\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return avg_loss, test_accuracy, predictions, probabilities_array\n",
    "\n",
    "####################################\n",
    "# 4) SAVE EPOCH FUNCTION\n",
    "####################################\n",
    "def save_epoch(\n",
    "    model_name,\n",
    "    model,\n",
    "    train_loss,\n",
    "    test_loss,\n",
    "    train_accuracy,\n",
    "    test_accuracy,\n",
    "    train_predictions,\n",
    "    test_predictions,\n",
    "    train_probabilities,     \n",
    "    test_probabilities,      \n",
    "    train_df,\n",
    "    test_df,\n",
    "    subregion_mapping,\n",
    "    save_weights=False\n",
    "):\n",
    "    os.makedirs(f'models/{model_name}', exist_ok=True)\n",
    "\n",
    "    # Metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'train_loss': [train_loss],\n",
    "        'test_loss': [test_loss],\n",
    "        'train_accuracy': [train_accuracy],\n",
    "        'test_accuracy': [test_accuracy]\n",
    "    })\n",
    "\n",
    "    # Reverse mapping for human-readable sub-region names\n",
    "    reverse_mapping = {v: k for k, v in subregion_mapping.items()}\n",
    "\n",
    "    # Single best-label predictions\n",
    "    train_df['model_prediction'] = [reverse_mapping[pred] for pred in train_predictions]\n",
    "    test_df['model_prediction'] = [reverse_mapping[pred] for pred in test_predictions]\n",
    "\n",
    "    # Add a probability column for each sub-region (e.g., \"prob_<sub-region>\")\n",
    "    for class_idx, class_name in reverse_mapping.items():\n",
    "        col_name_train = f'prob_{class_name}'\n",
    "        col_name_test = f'prob_{class_name}'\n",
    "\n",
    "        # For each row in train_probabilities, store the probability for class_idx\n",
    "        train_df[col_name_train] = [row[class_idx] for row in train_probabilities]\n",
    "        # For test\n",
    "        test_df[col_name_test] = [row[class_idx] for row in test_probabilities]\n",
    "\n",
    "    # Save model weights if required\n",
    "    if save_weights:\n",
    "        torch.save(model.state_dict(), f'models/{model_name}/model.pth')\n",
    "\n",
    "    # Save metrics and predictions to CSV\n",
    "    metrics_df.to_csv(f\"models/{model_name}/metrics.csv\", index=False)\n",
    "    train_df.to_csv(f\"models/{model_name}/train_predictions.csv\", index=False)\n",
    "    test_df.to_csv(f\"models/{model_name}/test_predictions.csv\", index=False)\n",
    "\n",
    "####################################\n",
    "# 6) TRAIN LOOP \n",
    "####################################\n",
    "def train_loop(\n",
    "    root_dir,\n",
    "    num_epochs=2,\n",
    "    batch_size=2,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    limit=None,\n",
    "    resnet_=resnet18,\n",
    "    weights_=ResNet18_Weights,\n",
    "    freeze = False,\n",
    "    augmented_data = False,\n",
    "):\n",
    "    # Prompt user for model name\n",
    "    model_name = input(\"Give model name: \")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data transformations\n",
    "    weights = weights_.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "\n",
    "    # Dataset\n",
    "    train_set = CustomDataset(\n",
    "        csv_file=\"train.csv\",\n",
    "        root_dir=root_dir,\n",
    "        transform=transform,\n",
    "        limit=limit,\n",
    "        augmented_data=augmented_data\n",
    "    )\n",
    "    test_set = CustomDataset(\n",
    "        csv_file=\"test.csv\",\n",
    "        root_dir=root_dir,\n",
    "        transform=transform,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Split indices for train/test\n",
    "    # train_idx, test_idx = train_test_split(\n",
    "    #     range(len(dataset)), test_size=0.2, random_state=42\n",
    "    # )\n",
    "    # train_set = Subset(dataset, train_idx)\n",
    "    # test_set = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # train/test DataFrames\n",
    "    train_df = train_set.data.reset_index(drop=True)\n",
    "    test_df = test_set.data.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_set)}\")\n",
    "    print(f\"Testing dataset size: {len(test_set)}\")\n",
    "\n",
    "    # Log missing files if any - this may be no longer necessary now that the file is fixed. \n",
    "    if train_set.missing_files:\n",
    "        print(f\"Missing files: {len(train_set.missing_files)}\")\n",
    "        with open('missing_files.log', 'w') as f:\n",
    "            for file in train_set.missing_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "\n",
    "    if test_set.missing_files:\n",
    "        print(f\"Missing files: {len(test_set.missing_files)}\")\n",
    "        with open('missing_files.log', 'w') as f:\n",
    "            for file in test_set.missing_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "\n",
    "    # Load chosen ResNet\n",
    "    resnet = resnet_(weights=weights_.DEFAULT)\n",
    "\n",
    "    # Freeze all layers except the last\n",
    "    if(freeze == True):\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    \n",
    "    # Modify the final FC layer\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, len(train_set.subregion_mapping))\n",
    "    resnet.to(device)\n",
    "\n",
    "    # Loss function & Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    training_times = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train step (returns probabilities)\n",
    "        train_loss, train_accuracy, train_predictions, train_probabilities = train(\n",
    "            resnet, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        # Test step (returns probabilities)\n",
    "        test_loss, test_accuracy, test_predictions, test_probabilities = test(\n",
    "            resnet, test_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        training_times.append({'epoch': epoch + 1, 'training_time': epoch_time})\n",
    "\n",
    "        # Save everything for this epoch\n",
    "        save_epoch(\n",
    "            model_name, \n",
    "            resnet,\n",
    "            train_loss, \n",
    "            test_loss,\n",
    "            train_accuracy, \n",
    "            test_accuracy,\n",
    "            train_predictions,\n",
    "            test_predictions,\n",
    "            train_probabilities,   \n",
    "            test_probabilities,    \n",
    "            train_df, \n",
    "            test_df, \n",
    "            train_set.subregion_mapping,\n",
    "            save_weights=(epoch == num_epochs - 1)  # only save on last epoch\n",
    "        )\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1} training time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # Save training times\n",
    "    training_times_df = pd.DataFrame(training_times)\n",
    "    training_times_df.to_csv(f\"models/{model_name}/training_times.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nTraining complete! Model and metrics saved to /models/{model_name}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loop(\n",
    "    root_dir='Streetview_Image_Dataset/processed',\n",
    "    num_epochs=2,\n",
    "    batch_size=8,  # for example\n",
    "    resnet_=resnet18, \n",
    "    weights_=ResNet18_Weights,\n",
    "    limit=120,\n",
    "    freeze=False,\n",
    "    augmented_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"coords_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latin America and the Caribbean    4955\n",
       "Northern America                   3141\n",
       "Eastern Europe                     2949\n",
       "Northern Europe                    2181\n",
       "Western Europe                     2026\n",
       "Sub-Saharan Africa                 1946\n",
       "Southern Europe                    1678\n",
       "Australia and New Zealand          1658\n",
       "South-eastern Asia                 1406\n",
       "Eastern Asia                       1290\n",
       "Western Asia                       1004\n",
       "Southern Asia                       886\n",
       "Name: sub-region, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sub-region\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
