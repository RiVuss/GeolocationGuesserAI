{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from torchvision import io\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'Streetview_Image_Dataset/'\n",
    "DATA_PATH = SOURCE + 'raw/'\n",
    "\n",
    "CSV_NAME = 'coordinates.csv'\n",
    "OUTPUT_PATH = SOURCE + 'processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.824885</td>\n",
       "      <td>-98.499517</td>\n",
       "      <td>0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.451752</td>\n",
       "      <td>-54.563937</td>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.496464</td>\n",
       "      <td>-47.460542</td>\n",
       "      <td>2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16.548678</td>\n",
       "      <td>-72.852778</td>\n",
       "      <td>3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35.010870</td>\n",
       "      <td>140.064397</td>\n",
       "      <td>4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude image_name\n",
       "0  20.824885  -98.499517      0.png\n",
       "1  -3.451752  -54.563937      1.png\n",
       "2 -23.496464  -47.460542      2.png\n",
       "3 -16.548678  -72.852778      3.png\n",
       "4 -35.010870  140.064397      4.png"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column_names = [\"latitude\", \"longitude\"]\n",
    "df = pd.read_csv(DATA_PATH + CSV_NAME,)\n",
    "df['image_name'] = df.index.astype('str') + '.png'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 41 (1657261813.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 47\u001b[1;36m\u001b[0m\n\u001b[1;33m    def process_chunk(chunk):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 41\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Helper functions for offline country/continent lookups\n",
    "# ---------------------------\n",
    "def latlon_to_country_code_batch(coords):\n",
    "    \"\"\"\n",
    "    Batch process reverse geocoding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = rg.search(coords)  # Batch process\n",
    "        return [res['cc'] for res in results]\n",
    "    except Exception as e:\n",
    "        print(f\"Batch reverse geocode failed. Error: {e}\")\n",
    "        return [None] * len(coords)\n",
    "\n",
    "def alpha2_to_country_name(alpha2):\n",
    "    \"\"\"\n",
    "    Converts ISO alpha-2 code to the official country name.\n",
    "    \"\"\"\n",
    "    country = pycountry.countries.get(alpha_2=alpha2)\n",
    "    return country.name if country else None\n",
    "\n",
    "def alpha2_to_continent(alpha2):\n",
    "    \"\"\"\n",
    "    Converts an ISO alpha-2 code to the name of the continent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        continent_code = pc.country_alpha2_to_continent_code(alpha2)\n",
    "        continent_map = {\n",
    "            \"AF\": \"Africa\",\n",
    "            \"NA\": \"North America\",\n",
    "            \"SA\": \"South America\",\n",
    "            \"OC\": \"Oceania\",\n",
    "            \"AS\": \"Asia\",\n",
    "            \"EU\": \"Europe\",\n",
    "            \"AN\": \"Antarctica\"\n",
    "        }\n",
    "        return continent_map.get(continent_code, None)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def alpha2_to_region(alpha2):\n",
    "    \n",
    "\n",
    "# ---------------------------\n",
    "# 2. Process entire dataset with parallel processing\n",
    "# ---------------------------\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Processes a chunk of the DataFrame, performing reverse geocoding\n",
    "    and mapping country/continent information.\n",
    "    \"\"\"\n",
    "    coords = list(zip(chunk[\"latitude\"], chunk[\"longitude\"]))\n",
    "    # Perform batch reverse geocoding\n",
    "    country_codes = latlon_to_country_code_batch(coords)\n",
    "    chunk[\"country_code\"] = country_codes\n",
    "    # Convert country codes to country names\n",
    "    chunk[\"country\"] = [alpha2_to_country_name(cc) for cc in country_codes]\n",
    "    # Convert country codes to continents\n",
    "    chunk[\"continent\"] = [alpha2_to_continent(cc) for cc in country_codes]\n",
    "    chunk[\"region\"] = chunk[\"country\"]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "chunk_size = max(1, len(df) // num_threads)  # Ensure chunk_size is at least 1\n",
    "chunks = [df.iloc[i:i + chunk_size].copy() for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Process chunks in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = list(executor.map(process_chunk, chunks))\n",
    "    \n",
    "df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "all_df = pd.read_csv('all.csv')\n",
    "\n",
    "df = df.merge(all_df[['alpha-2', 'sub-region']], left_on='country_code', right_on='alpha-2', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>sub-region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.824885</td>\n",
       "      <td>-98.499517</td>\n",
       "      <td>0.png</td>\n",
       "      <td>MX</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>North America</td>\n",
       "      <td>MX</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.451752</td>\n",
       "      <td>-54.563937</td>\n",
       "      <td>1.png</td>\n",
       "      <td>BR</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>South America</td>\n",
       "      <td>BR</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-23.496464</td>\n",
       "      <td>-47.460542</td>\n",
       "      <td>2.png</td>\n",
       "      <td>BR</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>South America</td>\n",
       "      <td>BR</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16.548678</td>\n",
       "      <td>-72.852778</td>\n",
       "      <td>3.png</td>\n",
       "      <td>PE</td>\n",
       "      <td>Peru</td>\n",
       "      <td>South America</td>\n",
       "      <td>PE</td>\n",
       "      <td>Latin America and the Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35.010870</td>\n",
       "      <td>140.064397</td>\n",
       "      <td>4.png</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude image_name country_code    country      continent  \\\n",
       "0  20.824885  -98.499517      0.png           MX     Mexico  North America   \n",
       "1  -3.451752  -54.563937      1.png           BR     Brazil  South America   \n",
       "2 -23.496464  -47.460542      2.png           BR     Brazil  South America   \n",
       "3 -16.548678  -72.852778      3.png           PE       Peru  South America   \n",
       "4 -35.010870  140.064397      4.png           AU  Australia        Oceania   \n",
       "\n",
       "  alpha-2                       sub-region  \n",
       "0      MX  Latin America and the Caribbean  \n",
       "1      BR  Latin America and the Caribbean  \n",
       "2      BR  Latin America and the Caribbean  \n",
       "3      PE  Latin America and the Caribbean  \n",
       "4      AU        Australia and New Zealand  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sub-region\n",
       "Melanesia                             3\n",
       "Central Asia                         41\n",
       "Northern Africa                      44\n",
       "Southern Asia                       886\n",
       "Western Asia                       1004\n",
       "Eastern Asia                       1290\n",
       "South-eastern Asia                 1406\n",
       "Australia and New Zealand          1658\n",
       "Southern Europe                    1678\n",
       "Sub-Saharan Africa                 1946\n",
       "Western Europe                     2026\n",
       "Northern Europe                    2181\n",
       "Eastern Europe                     2949\n",
       "Northern America                   3141\n",
       "Latin America and the Caribbean    4955\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_counts = df.groupby('sub-region').size().sort_values()\n",
    "region_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_regions = region_counts[region_counts < 500].index\n",
    "df = df[~df['sub-region'].isin(drop_regions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, output_path, image_path):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the train set to balance the classes\n",
    "def write_image(image, image_name, image_path=OUTPUT_PATH):\n",
    "    io.write_png(image, OUTPUT_PATH + image_name)\n",
    "\n",
    "def augment_data(image_name, input_image_path=DATA_PATH, output_image_path=OUTPUT_PATH):\n",
    "    image = io.read_image(input_image_path + image_name)\n",
    "    \n",
    "    transform = v2.Compose([\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.RandomRotation(degrees=30),\n",
    "        v2.ColorJitter(brightness=(0, 0.15), contrast=(0,0.15), saturation=(0, 0.15), hue=(0, 0.15)),\n",
    "    ])\n",
    "    \n",
    "    # apply augmentation to the image\n",
    "    augmented_image = transform(image)\n",
    "    # Placeholder for augmentation logic\n",
    "    return augmented_image\n",
    "\n",
    "\n",
    "def balance_classes(df):\n",
    "    image_name_index = len(df)\n",
    "    class_counts = df['sub-region'].value_counts()\n",
    "    \n",
    "    # Count the number of instances for each class\n",
    "    max_count = max(class_counts.values())\n",
    "    \n",
    "    for cls, count in class_counts.items():\n",
    "        if count < max_count:\n",
    "            # Number of samples needed to balance the class\n",
    "            num_samples_needed = max_count - count\n",
    "            df_minority = df[df['sub-region'] == cls]\n",
    "            \n",
    "            # Resample with replacement\n",
    "            df_resampled = df_minority.sample(n=num_samples_needed, replace=True)\n",
    "            augmented_images = df_resampled['image_name'].apply(augment_data, axis=1)\n",
    "            df_resampled['is_augmented'] = True\n",
    "            df_resampled['aumentation_source_image_name'] = df_resampled['image_name']\n",
    "            df_resampled['image_name'] = [str(image_name_index + i) + '.png' for i in range(1, num_samples_needed + 1)]\n",
    "            image_name_index += num_samples_needed\n",
    "            \n",
    "            df = pd.concat([df, df_resampled], ignore_index=True)\n",
    "            \n",
    "            # write the augmented images to disk\n",
    "            for i, image_name in enumerate(df_resampled['image_name']):\n",
    "                write_image(augmented_images[i], image_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Augment some images for classes that already have max_count\n",
    "            num_augmentations = int(0.25 * count)  # Augment 10% of the class images\n",
    "            df_majority = df[df['sub-region'] == cls]\n",
    "\n",
    "            # Randomly select images to augment\n",
    "            df_to_augment = df_majority.sample(n=num_augmentations)\n",
    "\n",
    "            augmented_images = df_to_augment['image_name'].apply(augment_data, axis=1)\n",
    "            df_to_augment['is_augmented'] = True\n",
    "            df_to_augment['augmentation_source_image_name'] = df_to_augment['image_name']\n",
    "            df_to_augment['image_name'] = [str(image_name_index + i) + '.png' for i in range(1, num_augmentations + 1)]\n",
    "            image_name_index += num_augmentations\n",
    "\n",
    "            df = pd.concat([df, df_to_augment], ignore_index=True)\n",
    "\n",
    "            # Write the augmented images to disk\n",
    "            for i, image_name in enumerate(df_to_augment['image_name']):\n",
    "                write_image(augmented_images[i], image_name)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_train\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_augmented\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maumentation_source_image_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m df_train\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_train['is_augmented'] = False\n",
    "df_train['aumentation_source_image_name'] = df_train['image_name']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coords_processed_large_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m coords_processed_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords_processed_large_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m all_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m coords_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_processed_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m all_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(all_file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Ensure country codes column is consistent in type\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\OneDrive\\University\\Courses\\Master\\Python\\uni\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\OneDrive\\University\\Courses\\Master\\Python\\uni\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\natha\\OneDrive\\University\\Courses\\Master\\Python\\uni\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\OneDrive\\University\\Courses\\Master\\Python\\uni\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\natha\\OneDrive\\University\\Courses\\Master\\Python\\uni\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coords_processed_large_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "coords_processed_file = 'coords_processed_large_dataset.csv'\n",
    "all_file = 'all.csv'\n",
    "\n",
    "# coords_df = pd.read_csv(coords_processed_file)\n",
    "all_df = pd.read_csv(all_file)\n",
    "\n",
    "# Ensure country codes column is consistent in type\n",
    "coords_df['country_code'] = df['country_code'].astype(str).str.strip()\n",
    "all_df['alpha-2'] = all_df['alpha-2'].astype(str).str.strip()\n",
    "\n",
    "# Merge the dataframes based on country codes\n",
    "merged_df = coords_df.merge(all_df[['alpha-2', 'sub-region']], left_on='country_code', right_on='alpha-2', how='left')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
