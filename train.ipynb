{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resnet 18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import io\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV and setting up transformations...\n",
      "Setting up dataset and data loaders...\n",
      "Splitting data into training and testing sets...\n",
      "Setting up data loaders...\n",
      "Loading ResNet18 and modifying the final layer...\n",
      "Setting up criterion and optimizer...\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "def read_image(image_path):\n",
    "    img = io.read_image(image_path)\n",
    "\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "    return transform(img)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Map continents to numeric labels\n",
    "        self.continent_mapping = {continent: idx for idx, continent in enumerate(self.data['continent'].unique())}\n",
    "        self.data['continent_label'] = self.data['continent'].map(self.continent_mapping)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_name'])\n",
    "        image = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data.iloc[idx]['continent_label']\n",
    "        return image, label\n",
    "\n",
    "def train_loop(csv_path, root_dir, num_epochs=15, batch_size=32, learning_rate=0.001, weight_decay=0.0001):\n",
    "    # Read CSV and set up transformations\n",
    "    print(\"Reading CSV and setting up transformations...\")\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    transform = weights.transforms()\n",
    "    \n",
    "    print(\"Setting up dataset and data loaders...\")\n",
    "    dataset = CustomDataset(csv_file=csv_path, root_dir=root_dir, transform=transform)\n",
    "    \n",
    "    # Train-test split\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    with open('train_idx.txt', 'w') as f:\n",
    "        for item in train_idx:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open('test_idx.txt', 'w') as f:\n",
    "        for item in test_idx:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    # Data loaders\n",
    "    print(\"Setting up data loaders...\")\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Load ResNet18 and modify the final layer\n",
    "    print(\"Loading ResNet18 and modifying the final layer...\")\n",
    "    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    for param in resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(num_features, 6)  # 6 output layers for 6 continents\n",
    "    \n",
    "    # Criterion and optimizer\n",
    "    print(\"Setting up criterion and optimizer...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(resnet.fc.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'train_accuracy', 'test_loss', 'test_accuracy'])\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(resnet, train_loader, optimizer, criterion)\n",
    "        test_loss, test_accuracy = test(resnet, test_loader, criterion)\n",
    "        \n",
    "        # Save model and metrics\n",
    "        metrics = metrics.append({'epoch': epoch, 'train_loss': train_loss, 'train_accuracy': train_accuracy, \n",
    "                                  'test_loss': test_loss, 'test_accuracy': test_accuracy}, ignore_index=True)\n",
    "        metrics.to_csv('metrics.csv', index=False)\n",
    "        torch.save(resnet.state_dict(), 'model.pth')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_accuracy:.4f}, \"\n",
    "              f\"Test Loss={test_loss:.4f}, Test Acc={test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data.csv' contains the relevant columns and images are stored in 'images/' directory\n",
    "train_loop(csv_path='coords_processed.csv', root_dir='data/archives', num_epochs=15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
